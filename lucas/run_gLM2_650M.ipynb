{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# login(token='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')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "model_name = 'tattabio/gLM2_650M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
    "pretrained_model = AutoModel.from_pretrained(model_name,trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new token to the tokenizer\n",
    "next_token = '<next>'\n",
    "\n",
    "# create the finetuning dataset\n",
    "# for overlap in ['overlap0','overlap100','overlap200','overlap300','overlap400','overlap500']:\n",
    "overlap = 'overlap100'\n",
    "dataset = load_dataset('bioLLM/evo_ecoli_len1000',split=overlap)\n",
    "seqs = dataset['sequence'][:-1]\n",
    "seqs = [f'<+>{seq.lower()}' for seq in seqs]\n",
    "\n",
    "positive_label = 1\n",
    "positive_pairs = []\n",
    "for i in range(1,len(seqs)):\n",
    "    positive_pairs.append( (f'{seqs[i-1]}{next_token}{seqs[i]}',positive_label) )\n",
    "\n",
    "idxs = []\n",
    "for _ in range(len(seqs)-1):\n",
    "    while True:\n",
    "        tup = tuple(random.randint(0, len(seqs)-1) for _ in range(2))\n",
    "        if abs(tup[0] - tup[1]) > 1 and tup[0] != tup[1]:\n",
    "            idxs.append(tup)\n",
    "            break\n",
    "\n",
    "negative_label = 0\n",
    "negative_pairs = [ (f'{seqs[idx[0]]}{next_token}{seqs[idx[1]]}',negative_label) for idx in idxs]\n",
    "\n",
    "assert len(positive_pairs) == len(negative_pairs)\n",
    "all_pairs = positive_pairs + negative_pairs\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "new_tokens = [next_token]\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n",
    "\n",
    "new_tokens = [next_token, positive_label, negative_label]\n",
    "new_vocab_size = pretrained_model.tok_embeddings.num_embeddings + len(new_tokens)\n",
    "new_embeddings = torch.nn.Embedding(new_vocab_size, pretrained_model.tok_embeddings.embedding_dim)\n",
    "new_embeddings.weight.data[:pretrained_model.tok_embeddings.num_embeddings] = pretrained_model.tok_embeddings.weight.data\n",
    "pretrained_model.tok_embeddings = new_embeddings\n",
    "\n",
    "tokenized_data = [tokenizer(pair[0]) for pair in all_pairs]\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': [x['input_ids'] for x in tokenized_data],\n",
    "    'token_type_ids': [x['token_type_ids'] for x in tokenized_data],\n",
    "    'attention_mask': [x['attention_mask'] for x in tokenized_data],\n",
    "    'label': [pair[1] for pair in all_pairs]\n",
    "})\n",
    "test_dataset = test_dataset.class_encode_column('label')\n",
    "test_dataset = test_dataset.train_test_split(test_size=0.2,seed=0,stratify_by_column='label')\n",
    "\n",
    "print(overlap, pd.DataFrame(test_dataset['test']['label']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_labels):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.classifier = nn.Linear(1280, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:,0]\n",
    "        logits = self.classifier(pooled_output)       \n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "model = ClassificationModel(pretrained_model=pretrained_model, num_labels=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    run_name=overlap+model_name.split('/')[-1]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_dataset['train'],  \n",
    "    eval_dataset=test_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
